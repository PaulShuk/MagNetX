{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Network Training**\n",
        "\n",
        "This tutorial demonstrates how to train the double LSTM-based model for the seqeunce-to-scalar future hysteresis step prediction. The model may serve as a good starting point for the neural network based transient magnetic modeling. The network model will be trained based on 3C90_Training_Tutorial.h5 file and saved as a state dictionary (.sd) file. The training data is a all frequency inclusive, 50-sequences per freqeuncy dataset with each seuqence containing only 1000 randomly selected time steps for training, operating under all availble temepratures, and flux excitations.\n",
        "\n",
        "\n",
        "# **Step 0: Import Packages**\n",
        "\n",
        "In this step we import the important packages that are necessary for the training."
      ],
      "metadata": {
        "id": "Tg0WAGktMOVH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oE_3CSIzfC1d",
        "outputId": "e769f849-6e2e-4c8d-f801-1373fbd9e567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "import csv\n",
        "import time\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Define Network Structure**\n",
        "The structure of the duel LSTM-based encoder-projector-decoder neural network are defined here. Refer to the PyTorch document for more details."
      ],
      "metadata": {
        "id": "ENIjgL-si0I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model structures and functions\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.lstm_B = nn.LSTM(1, 8, num_layers=1, batch_first=True, bidirectional=False)\n",
        "\n",
        "        self.lstm_H = nn.LSTM(1, 8, num_layers=1, batch_first=True, bidirectional=False)\n",
        "\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(8 *2 + 2 , 8 *2 + 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8 *2 + 2, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8 , 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, seq_B: Tensor, seq_H: Tensor, scal: Tensor, T: Tensor, device) -> Tensor:\n",
        "\n",
        "        seq_B = seq_B.float()\n",
        "        seq_H = seq_H.float()\n",
        "        scal = scal.float()\n",
        "        T = T.float()\n",
        "        x_B, _ = self.lstm_B(seq_B)\n",
        "        x_B = x_B[:, -1, :]\n",
        "        x_H, _ = self.lstm_H(seq_H)\n",
        "        x_H = x_H[:, -1, :]\n",
        "\n",
        "        # print(x_B.size())\n",
        "        # print(x_H.size())\n",
        "        # print(scal.size())\n",
        "        output = self.projector(torch.cat((scal, T, x_B, x_H), dim=1))\n",
        "        output = output.to(device)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "dYMQPMknLUPN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Load the Training Dataset**\n",
        "\n",
        "Dataset needs to be processed before trianing. This includes the normalization of the data. In this part, we load and pre-process the dataset for the network training and testing. In this demo, a small dataset measured with 3C90 ferrite material containing all frequency, temperature, flux excitations is used. The full dataset can be downloaded from the MagNet Challenge GitHub repository."
      ],
      "metadata": {
        "id": "2CXI2TIOpNJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "def get_dataset(data_length=80):\n",
        "\n",
        "    # Change the directory to where the training data is located. The file used is h5py file. For more inofmration, please visit https://docs.h5py.org/en/latest/build.html\n",
        "    with h5py.File('/content/drive/MyDrive/MLTran/3C90_Training_Tutorial.h5', 'r') as file:\n",
        "        print(\"keys:\", list(file.keys()))\n",
        "\n",
        "        B_list = []\n",
        "        H_list = []\n",
        "        B_scal_list = []\n",
        "        H_out_list = []\n",
        "        T_scal_list = []\n",
        "\n",
        "        for i in range(1, 8):  # i from 1 to 7, for reading all frequency datasets\n",
        "            B_list.append(file[f'B_seq_f_{i}'][:])\n",
        "            H_list.append(file[f'H_seq_f_{i}'][:])\n",
        "            B_scal_list.append(file[f'B_scal_{i}'][:])\n",
        "            H_out_list.append(file[f'H_scal_{i}'][:])\n",
        "            T_scal_list.append(file[f'T_{i}'][:])\n",
        "\n",
        "\n",
        "        # Now concatenate all of them after the loop\n",
        "        B = np.concatenate(B_list, axis=0)\n",
        "        H = np.concatenate(H_list, axis=0)\n",
        "        B_scal = np.concatenate(B_scal_list, axis=0)\n",
        "        H_out = np.concatenate(H_out_list, axis=0)\n",
        "        T_scal = np.concatenate(T_scal_list, axis=0)\n",
        "\n",
        "    print(\"Data Loading Initiated\")\n",
        "\n",
        "\n",
        "    B = np.array(B)\n",
        "    H = np.array(H)\n",
        "    B_scal = np.array(B_scal)\n",
        "    H_out = np.array(H_out)\n",
        "    T_scal = np.array(T_scal)\n",
        "\n",
        "    B_scal = B_scal.reshape(-1,1)\n",
        "    B_scal = torch.from_numpy(B_scal)\n",
        "    T_scal = T_scal.reshape(-1,1)\n",
        "    T_scal = torch.from_numpy(T_scal)\n",
        "\n",
        "    in_B = torch.from_numpy(B).float().view(-1,data_length,1)\n",
        "    in_H = torch.from_numpy(H).float().view(-1,data_length,1)\n",
        "    out = torch.from_numpy(H_out).float().view(-1,1)\n",
        "\n",
        "\n",
        "    # Save the normalized parameters to JSON file\n",
        "    with open('/content/drive/MyDrive/MLTran/Normalization_Params.json', 'w') as f:\n",
        "        json.dump({'mean_B': torch.mean(in_B).tolist(),\n",
        "                    'std_B': torch.std(in_B).tolist(),\n",
        "                    'mean_H': torch.mean(in_H).tolist(),\n",
        "                    'std_H': torch.std(in_H).tolist(),\n",
        "                    'mean_out': torch.mean(out).tolist(),\n",
        "                    'std_out': torch.std(out).tolist(),\n",
        "                    'mean_Scal': torch.mean(B_scal).tolist(),\n",
        "                    'std_Scal': torch.std(B_scal).tolist(),\n",
        "                    'mean_T': torch.mean(T_scal).tolist(),\n",
        "                    'std_T': torch.std(T_scal).tolist()},f)\n",
        "\n",
        "    B_scal = (B_scal-torch.mean(B_scal))/torch.std(B_scal)\n",
        "    T_scal = (T_scal-torch.mean(T_scal))/torch.std(T_scal)\n",
        "    in_B = (in_B-torch.mean(in_B))/torch.std(in_B)\n",
        "    in_H = (in_H-torch.mean(in_H))/torch.std(in_H)\n",
        "    out = (out-torch.mean(out))/torch.std(out)\n",
        "\n",
        "\n",
        "\n",
        "    print(in_B.size())\n",
        "    print(in_H.size())\n",
        "    print(B_scal.size())\n",
        "    print(out.size())\n",
        "\n",
        "\n",
        "\n",
        "    return torch.utils.data.TensorDataset(in_B, in_H , B_scal, T_scal, out)"
      ],
      "metadata": {
        "id": "4O__WD8JLUjt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Training the Model**\n",
        "\n",
        "The loaded dataset is randomly split into training set, validation set, and test set. The output of the training is saved into the state dictionary file (.sd) containing all the trained parameter values. In this exmaple however, the test set data is from a seperate file."
      ],
      "metadata": {
        "id": "DJcX3ZPesurO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defind parameters\n",
        "# Define the memory length your data is saved with\n",
        "data_length = 80\n",
        "\n",
        "# Count the number of parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# Config the model training\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Reproducibility\n",
        "    random.seed(1)\n",
        "    np.random.seed(1)\n",
        "    torch.manual_seed(1)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    NUM_EPOCH = 200\n",
        "    BATCH_SIZE = 2048\n",
        "    DECAY_EPOCH = 50\n",
        "    DECAY_RATIO = 0.6\n",
        "    LR_INI = 0.001\n",
        "\n",
        "    times = []\n",
        "    results = []\n",
        "\n",
        "    # Select GPU as default device\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = get_dataset()\n",
        "\n",
        "    # Split the dataset\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    valid_size = int(0.1 * len(dataset))\n",
        "    test_size = len(dataset)- train_size- valid_size\n",
        "    train_dataset, valid_dataset, test_dataset= torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
        "    kwargs = {'num_workers': 0, 'pin_memory': True, 'pin_memory_device': \"cuda\"}\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
        "    test_loader =  torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
        "    trainData = list(train_loader)\n",
        "    validData = list(valid_loader)\n",
        "    testData = list(test_loader) # Not used in training, have a seperate dataset for the test file\n",
        "\n",
        "    # Setup network\n",
        "    net = Net().to(device)\n",
        "\n",
        "    # Log the number of parameters\n",
        "    print(\"Number of parameters: \", count_parameters(net))\n",
        "    print(\"Number of parameters: \", count_parameters(net.lstm_B))\n",
        "    print(\"Number of parameters: \", count_parameters(net.projector))\n",
        "\n",
        "    # Setup optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LR_INI)\n",
        "\n",
        "    # Record initial time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Setup the vairables for the model saving\n",
        "    min_loss = 1e16\n",
        "    min_Num = 0\n",
        "    num_saved = 0\n",
        "\n",
        "\n",
        "    # Train the network\n",
        "    for epoch_i in range(NUM_EPOCH):\n",
        "\n",
        "        start_epoch = time.time()\n",
        "\n",
        "        # Train for one epoch\n",
        "        epoch_train_loss = 0\n",
        "        net.train()\n",
        "        optimizer.param_groups[0]['lr'] = LR_INI* (DECAY_RATIO ** (0+ epoch_i // DECAY_EPOCH))\n",
        "\n",
        "        for in_B, in_H, B_scal, T_scal, out in trainData:\n",
        "            optimizer.zero_grad()\n",
        "            output = net(seq_B=in_B.to(device), seq_H = in_H.to(device), scal = B_scal.to(device), T = T_scal.to(device) , device=device)\n",
        "            loss = criterion(output, out.to(device))\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "        # Compute Validation Loss\n",
        "        with torch.no_grad():\n",
        "            epoch_valid_loss = 0\n",
        "            for in_B, in_H, B_scal, T_scal, out in validData:\n",
        "                output_valid = net(seq_B=in_B.to(device), seq_H = in_H.to(device), scal = B_scal.to(device), T = T_scal.to(device), device=device)\n",
        "                loss = criterion(output_valid, out.to(device))\n",
        "\n",
        "\n",
        "                epoch_valid_loss += loss.item()\n",
        "\n",
        "\n",
        "        end_epoch = time.time()\n",
        "        times.append(end_epoch-start_epoch)\n",
        "\n",
        "        # Print the training and validation loss and save them in a csv file\n",
        "        if (epoch_i+1)%50 == 0:\n",
        "          print(f\"Epoch {epoch_i+1:2d} \"\n",
        "              f\"Train {epoch_train_loss / len(train_dataset) * 1e5:.5f} \"\n",
        "              f\"Valid {epoch_valid_loss / len(valid_dataset) * 1e5:.5f}\")\n",
        "          results.append([epoch_i+1, epoch_train_loss / len(train_dataset) * 1e5, epoch_valid_loss / len(valid_dataset) * 1e5])\n",
        "          with open(\"/content/drive/MyDrive/MLTran/Training_.csv\", \"w\") as f:\n",
        "              writer = csv.writer(f)\n",
        "              writer.writerows(results)\n",
        "\n",
        "\n",
        "        # Save the model parameters based on the lowest subsequent validation loss\n",
        "        if (min_loss> (epoch_valid_loss / len(valid_dataset))):\n",
        "          min_loss = epoch_valid_loss / len(valid_dataset)\n",
        "\n",
        "          #print(\"Model Updated:\", num_saved,\" Saved\")\n",
        "          torch.save(net.state_dict(), \"/content/drive/MyDrive/MLTran/Model/Model_LSTM_\"+str(num_saved)+\".sd\")\n",
        "          num_saved = num_saved + 1\n",
        "\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Total Time Elapsed: {elapsed}\")\n",
        "    print(f\"Average time per Epoch: {sum(times)/NUM_EPOCH}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "wmMVUUOCusfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce5a9420-f0d9-4aea-a656-2e39d87cbbc7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys: ['B_scal_1', 'B_scal_2', 'B_scal_3', 'B_scal_4', 'B_scal_5', 'B_scal_6', 'B_scal_7', 'B_seq_f_1', 'B_seq_f_2', 'B_seq_f_3', 'B_seq_f_4', 'B_seq_f_5', 'B_seq_f_6', 'B_seq_f_7', 'H_scal_1', 'H_scal_2', 'H_scal_3', 'H_scal_4', 'H_scal_5', 'H_scal_6', 'H_scal_7', 'H_seq_f_1', 'H_seq_f_2', 'H_seq_f_3', 'H_seq_f_4', 'H_seq_f_5', 'H_seq_f_6', 'H_seq_f_7', 'T_1', 'T_2', 'T_3', 'T_4', 'T_5', 'T_6', 'T_7']\n",
            "Data Loading Initiated\n",
            "torch.Size([350000, 80, 1])\n",
            "torch.Size([350000, 80, 1])\n",
            "torch.Size([350000, 1])\n",
            "torch.Size([350000, 1])\n",
            "Number of parameters:  1239\n",
            "Number of parameters:  352\n",
            "Number of parameters:  535\n",
            "Epoch 50 Train 0.00837 Valid 0.01105\n",
            "Epoch 100 Train 0.00686 Valid 0.00780\n",
            "Epoch 150 Train 0.00651 Valid 0.00685\n",
            "Epoch 200 Train 0.00641 Valid 0.00654\n",
            "Total Time Elapsed: 218.83896374702454\n",
            "Average time per Epoch: 1.0897246634960174\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}